// File generated by hadoop record compiler. Do not edit.
package com.yahoo.ccdi.fetl;

public class ManualETLValue_worked extends org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable {
  private static final org.apache.hadoop.record.meta.RecordTypeInfo _rio_recTypeInfo;
  private static org.apache.hadoop.record.meta.RecordTypeInfo _rio_rtiFilter;
  private static int[] _rio_rtiFilterFields;
  private static int writeIndx = 0;
  static {
    _rio_recTypeInfo = new org.apache.hadoop.record.meta.RecordTypeInfo("ManualETLValue");
    _rio_recTypeInfo.addField("filterTag", org.apache.hadoop.record.meta.TypeID.LongTypeID);
    _rio_recTypeInfo.addField("dhrTag", org.apache.hadoop.record.meta.TypeID.LongTypeID);
    _rio_recTypeInfo.addField("transformErrorTag", org.apache.hadoop.record.meta.TypeID.LongTypeID);
    _rio_recTypeInfo.addField("simpleFields", new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.StringTypeID, org.apache.hadoop.record.meta.TypeID.BufferTypeID));
    _rio_recTypeInfo.addField("mapFields", new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.StringTypeID, new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.StringTypeID, org.apache.hadoop.record.meta.TypeID.BufferTypeID)));
    _rio_recTypeInfo.addField("mapListFields", new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.StringTypeID, new org.apache.hadoop.record.meta.VectorTypeID(new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.StringTypeID, org.apache.hadoop.record.meta.TypeID.BufferTypeID))));
  }
  
  private long filterTag;
  private long dhrTag;
  private long transformErrorTag;
  private java.util.TreeMap<String,org.apache.hadoop.record.Buffer> simpleFields;
  private java.util.TreeMap<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>> mapFields;
  private java.util.TreeMap<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>> mapListFields;
  public ManualETLValue_worked() { }
  public ManualETLValue_worked(
    final long filterTag,
    final long dhrTag,
    final long transformErrorTag,
    final java.util.TreeMap<String,org.apache.hadoop.record.Buffer> simpleFields,
    final java.util.TreeMap<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>> mapFields,
    final java.util.TreeMap<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>> mapListFields) {
    this.filterTag = filterTag;
    this.dhrTag = dhrTag;
    this.transformErrorTag = transformErrorTag;
    this.simpleFields = simpleFields;
    this.mapFields = mapFields;
    this.mapListFields = mapListFields;
    this.serialize();
  }
  public static org.apache.hadoop.record.meta.RecordTypeInfo getTypeInfo() {
    return _rio_recTypeInfo;
  }
  public static void setTypeFilter(org.apache.hadoop.record.meta.RecordTypeInfo rti) {
    if (null == rti) return;
    _rio_rtiFilter = rti;
    _rio_rtiFilterFields = null;
  }
  private static void setupRtiFields()
  {
    if (null == _rio_rtiFilter) return;
    // we may already have done this
    if (null != _rio_rtiFilterFields) return;
    int _rio_i, _rio_j;
    _rio_rtiFilterFields = new int [_rio_rtiFilter.getFieldTypeInfos().size()];
    for (_rio_i=0; _rio_i<_rio_rtiFilterFields.length; _rio_i++) {
      _rio_rtiFilterFields[_rio_i] = 0;
    }
    java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_itFilter = _rio_rtiFilter.getFieldTypeInfos().iterator();
    _rio_i=0;
    while (_rio_itFilter.hasNext()) {
      org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfoFilter = _rio_itFilter.next();
      java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_it = _rio_recTypeInfo.getFieldTypeInfos().iterator();
      _rio_j=1;
      while (_rio_it.hasNext()) {
        org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfo = _rio_it.next();
        if (_rio_tInfo.equals(_rio_tInfoFilter)) {
          _rio_rtiFilterFields[_rio_i] = _rio_j;
          break;
        }
        _rio_j++;
      }
      _rio_i++;
    }
  }
  public long getFilterTag() {
    return filterTag;
  }
  public void setFilterTag(final long filterTag) {
    this.filterTag=filterTag;
  }
  public long getDhrTag() {
    return dhrTag;
  }
  public void setDhrTag(final long dhrTag) {
    this.dhrTag=dhrTag;
  }
  public long getTransformErrorTag() {
    return transformErrorTag;
  }
  public void setTransformErrorTag(final long transformErrorTag) {
    this.transformErrorTag=transformErrorTag;
  }
  public java.util.TreeMap<String,org.apache.hadoop.record.Buffer> getSimpleFields() {
    return simpleFields;
  }
  public void setSimpleFields(final java.util.TreeMap<String,org.apache.hadoop.record.Buffer> simpleFields) {
    this.simpleFields=simpleFields;
  }
  public java.util.TreeMap<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>> getMapFields() {
    return mapFields;
  }
  public void setMapFields(final java.util.TreeMap<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>> mapFields) {
    this.mapFields=mapFields;
  }
  public java.util.TreeMap<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>> getMapListFields() {
    return mapListFields;
  }
  public void setMapListFields(final java.util.TreeMap<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>> mapListFields) {
    this.mapListFields=mapListFields;
  }
  public void serialize() {
    int writeIndx = 0;
    try {
      com.yahoo.ccdi.fetl.RcUtil.writeLong(this, filterTag, writeIndx++);
      com.yahoo.ccdi.fetl.RcUtil.writeLong(this, dhrTag, writeIndx++);
      com.yahoo.ccdi.fetl.RcUtil.writeLong(this, transformErrorTag, writeIndx++);
      {
        com.yahoo.ccdi.fetl.RcUtil.writeInt(this, simpleFields.size(), writeIndx++);
        java.util.Set<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_es1 = simpleFields.entrySet();
        for(java.util.Iterator<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_midx1 = _rio_es1.iterator(); _rio_midx1.hasNext();) {
          java.util.Map.Entry<String,org.apache.hadoop.record.Buffer> _rio_me1 = _rio_midx1.next();
          String _rio_k1 = _rio_me1.getKey();
          org.apache.hadoop.record.Buffer _rio_v1 = _rio_me1.getValue();
          com.yahoo.ccdi.fetl.RcUtil.writeString(this, _rio_k1, writeIndx++);
          com.yahoo.ccdi.fetl.RcUtil.writeBuffer(this, _rio_v1, writeIndx++);
        }
      }
      {
        com.yahoo.ccdi.fetl.RcUtil.writeInt(this, mapFields.size(), writeIndx++);
        java.util.Set<java.util.Map.Entry<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>> _rio_es1 = mapFields.entrySet();
        for(java.util.Iterator<java.util.Map.Entry<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>> _rio_midx1 = _rio_es1.iterator(); _rio_midx1.hasNext();) {
          java.util.Map.Entry<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>> _rio_me1 = _rio_midx1.next();
          String _rio_k1 = _rio_me1.getKey();
          java.util.TreeMap<String,org.apache.hadoop.record.Buffer> _rio_v1 = _rio_me1.getValue();
          com.yahoo.ccdi.fetl.RcUtil.writeString(this, _rio_k1, writeIndx++);
          {
            com.yahoo.ccdi.fetl.RcUtil.writeInt(this, _rio_v1.size(), writeIndx++);
            java.util.Set<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_es2 = _rio_v1.entrySet();
            for(java.util.Iterator<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_midx2 = _rio_es2.iterator(); _rio_midx2.hasNext();) {
              java.util.Map.Entry<String,org.apache.hadoop.record.Buffer> _rio_me2 = _rio_midx2.next();
              String _rio_k2 = _rio_me2.getKey();
              org.apache.hadoop.record.Buffer _rio_v2 = _rio_me2.getValue();
              com.yahoo.ccdi.fetl.RcUtil.writeString(this, _rio_k2, writeIndx++);
              com.yahoo.ccdi.fetl.RcUtil.writeBuffer(this, _rio_v2, writeIndx++);
            }
          }
        }
      }
      {
        com.yahoo.ccdi.fetl.RcUtil.writeInt(this, mapListFields.size(), writeIndx++);
        java.util.Set<java.util.Map.Entry<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>>> _rio_es1 = mapListFields.entrySet();
        for(java.util.Iterator<java.util.Map.Entry<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>>> _rio_midx1 = _rio_es1.iterator(); _rio_midx1.hasNext();) {
          java.util.Map.Entry<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>> _rio_me1 = _rio_midx1.next();
          String _rio_k1 = _rio_me1.getKey();
          java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>> _rio_v1 = _rio_me1.getValue();
          com.yahoo.ccdi.fetl.RcUtil.writeString(this, _rio_k1, writeIndx++);
          {
            com.yahoo.ccdi.fetl.RcUtil.writeInt(this, _rio_v1.size(), writeIndx++);
            int _rio_len1 = _rio_v1.size();
            for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len1; _rio_vidx1++) {
              java.util.TreeMap<String,org.apache.hadoop.record.Buffer> _rio_e1 = _rio_v1.get(_rio_vidx1);
              {
                com.yahoo.ccdi.fetl.RcUtil.writeInt(this, _rio_e1.size(), writeIndx++);
                java.util.Set<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_es2 = _rio_e1.entrySet();
                for(java.util.Iterator<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_midx2 = _rio_es2.iterator(); _rio_midx2.hasNext();) {
                  java.util.Map.Entry<String,org.apache.hadoop.record.Buffer> _rio_me2 = _rio_midx2.next();
                  String _rio_k2 = _rio_me2.getKey();
                  org.apache.hadoop.record.Buffer _rio_v2 = _rio_me2.getValue();
                  com.yahoo.ccdi.fetl.RcUtil.writeString(this, _rio_k2, writeIndx++);
                  com.yahoo.ccdi.fetl.RcUtil.writeBuffer(this, _rio_v2, writeIndx++);
                }
              }
            }
          }
        }
      }
    } catch(java.io.IOException e) {
      e.printStackTrace();
    }
  }
  public void deserialize(org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable bra){
    int readIndx = 0;
    try {
      filterTag=com.yahoo.ccdi.fetl.RcUtil.readLong(bra, readIndx++);
      dhrTag=com.yahoo.ccdi.fetl.RcUtil.readLong(bra, readIndx++);
      transformErrorTag=com.yahoo.ccdi.fetl.RcUtil.readLong(bra, readIndx++);
      {
        int _rio_midx1= com.yahoo.ccdi.fetl.RcUtil.readInt(bra, readIndx++);
        simpleFields=new java.util.TreeMap<String,org.apache.hadoop.record.Buffer>();
        for ( int _rio_tmp1 = 0; _rio_tmp1< _rio_midx1; _rio_tmp1++) {
          String _rio_k1;
          _rio_k1=com.yahoo.ccdi.fetl.RcUtil.readString(bra, readIndx++);
          org.apache.hadoop.record.Buffer _rio_v1;
          _rio_v1=com.yahoo.ccdi.fetl.RcUtil.readBuffer(bra, readIndx++);
          simpleFields.put(_rio_k1,_rio_v1);
        }
      }
      {
        int _rio_midx1= com.yahoo.ccdi.fetl.RcUtil.readInt(bra, readIndx++);
        mapFields=new java.util.TreeMap<String,java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>();
        for ( int _rio_tmp1 = 0; _rio_tmp1< _rio_midx1; _rio_tmp1++) {
          String _rio_k1;
          _rio_k1=com.yahoo.ccdi.fetl.RcUtil.readString(bra, readIndx++);
          java.util.TreeMap<String,org.apache.hadoop.record.Buffer> _rio_v1;
          {
            int _rio_midx2= com.yahoo.ccdi.fetl.RcUtil.readInt(bra, readIndx++);
            _rio_v1=new java.util.TreeMap<String,org.apache.hadoop.record.Buffer>();
            for ( int _rio_tmp2 = 0; _rio_tmp2< _rio_midx2; _rio_tmp2++) {
              String _rio_k2;
              _rio_k2=com.yahoo.ccdi.fetl.RcUtil.readString(bra, readIndx++);
              org.apache.hadoop.record.Buffer _rio_v2;
              _rio_v2=com.yahoo.ccdi.fetl.RcUtil.readBuffer(bra, readIndx++);
              _rio_v1.put(_rio_k2,_rio_v2);
            }
          }
          mapFields.put(_rio_k1,_rio_v1);
        }
      }
      {
        int _rio_midx1= com.yahoo.ccdi.fetl.RcUtil.readInt(bra, readIndx++);
        mapListFields=new java.util.TreeMap<String,java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>>();
        for ( int _rio_tmp1 = 0; _rio_tmp1< _rio_midx1; _rio_tmp1++) {
          String _rio_k1;
          _rio_k1=com.yahoo.ccdi.fetl.RcUtil.readString(bra, readIndx++);
          java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>> _rio_v1;
          {
            int _rio_len1 = com.yahoo.ccdi.fetl.RcUtil.readInt(bra, readIndx++);
            _rio_v1=new java.util.ArrayList<java.util.TreeMap<String,org.apache.hadoop.record.Buffer>>();
            for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len1; _rio_vidx1++) {
              java.util.TreeMap<String,org.apache.hadoop.record.Buffer> _rio_e1;
              {
                int _rio_midx2= com.yahoo.ccdi.fetl.RcUtil.readInt(bra, readIndx++);
                _rio_e1=new java.util.TreeMap<String,org.apache.hadoop.record.Buffer>();
                for ( int _rio_tmp2 = 0; _rio_tmp2< _rio_midx2; _rio_tmp2++) {
                  String _rio_k2;
                  _rio_k2=com.yahoo.ccdi.fetl.RcUtil.readString(bra, readIndx++);
                  org.apache.hadoop.record.Buffer _rio_v2;
                  _rio_v2=com.yahoo.ccdi.fetl.RcUtil.readBuffer(bra, readIndx++);
                  _rio_e1.put(_rio_k2,_rio_v2);
                }
              }
              _rio_v1.add(_rio_e1);
            }
          }
          mapListFields.put(_rio_k1,_rio_v1);
        }
      }
    } catch(java.io.IOException e) {
      e.printStackTrace();
    }
  }
}
