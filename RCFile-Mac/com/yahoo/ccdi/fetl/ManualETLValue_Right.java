// File generated by hadoop record compiler. Do not edit.
package com.yahoo.ccdi.fetl;

public class ManualETLValue_Right extends org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable {
  private static final org.apache.hadoop.record.meta.RecordTypeInfo _rio_recTypeInfo;
  private static org.apache.hadoop.record.meta.RecordTypeInfo _rio_rtiFilter;
  private static int[] _rio_rtiFilterFields;
  private static int index = 0;
  static {
    _rio_recTypeInfo = new org.apache.hadoop.record.meta.RecordTypeInfo("ManualETLValue");
    _rio_recTypeInfo.addField("filterTag", org.apache.hadoop.record.meta.TypeID.LongTypeID);
    _rio_recTypeInfo.addField("dhrTag", org.apache.hadoop.record.meta.TypeID.LongTypeID);
    _rio_recTypeInfo.addField("transformErrorTag", org.apache.hadoop.record.meta.TypeID.LongTypeID);
    _rio_recTypeInfo.addField("simpleFields", new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.StringTypeID, org.apache.hadoop.record.meta.TypeID.BufferTypeID));
  }
  
  private long filterTag;
  private long dhrTag;
  private long transformErrorTag;
  private java.util.TreeMap<String,org.apache.hadoop.record.Buffer> simpleFields;
  public ManualETLValue_Right() { }
  public ManualETLValue_Right(
    final long filterTag,
    final long dhrTag,
    final long transformErrorTag,
    final java.util.TreeMap<String,org.apache.hadoop.record.Buffer> simpleFields) {
    this.filterTag = filterTag;
    this.dhrTag = dhrTag;
    this.transformErrorTag = transformErrorTag;
    this.simpleFields = simpleFields;
    this.serialize();
  }
  public static org.apache.hadoop.record.meta.RecordTypeInfo getTypeInfo() {
    return _rio_recTypeInfo;
  }
  public static void setTypeFilter(org.apache.hadoop.record.meta.RecordTypeInfo rti) {
    if (null == rti) return;
    _rio_rtiFilter = rti;
    _rio_rtiFilterFields = null;
  }
  private static void setupRtiFields()
  {
    if (null == _rio_rtiFilter) return;
    // we may already have done this
    if (null != _rio_rtiFilterFields) return;
    int _rio_i, _rio_j;
    _rio_rtiFilterFields = new int [_rio_rtiFilter.getFieldTypeInfos().size()];
    for (_rio_i=0; _rio_i<_rio_rtiFilterFields.length; _rio_i++) {
      _rio_rtiFilterFields[_rio_i] = 0;
    }
    java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_itFilter = _rio_rtiFilter.getFieldTypeInfos().iterator();
    _rio_i=0;
    while (_rio_itFilter.hasNext()) {
      org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfoFilter = _rio_itFilter.next();
      java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_it = _rio_recTypeInfo.getFieldTypeInfos().iterator();
      _rio_j=1;
      while (_rio_it.hasNext()) {
        org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfo = _rio_it.next();
        if (_rio_tInfo.equals(_rio_tInfoFilter)) {
          _rio_rtiFilterFields[_rio_i] = _rio_j;
          break;
        }
        _rio_j++;
      }
      _rio_i++;
    }
  }
  public long getFilterTag() {
    return filterTag;
  }
  public void setFilterTag(final long filterTag) {
    this.filterTag=filterTag;
  }
  public long getDhrTag() {
    return dhrTag;
  }
  public void setDhrTag(final long dhrTag) {
    this.dhrTag=dhrTag;
  }
  public long getTransformErrorTag() {
    return transformErrorTag;
  }
  public void setTransformErrorTag(final long transformErrorTag) {
    this.transformErrorTag=transformErrorTag;
  }
  public java.util.TreeMap<String,org.apache.hadoop.record.Buffer> getSimpleFields() {
    return simpleFields;
  }
  public void setSimpleFields(final java.util.TreeMap<String,org.apache.hadoop.record.Buffer> simpleFields) {
    this.simpleFields=simpleFields;
  }
  public void serialize() {
    try {
      int indx = 0;
      com.yahoo.ccdi.fetl.RcUtil.writeLong(this, filterTag, indx++);
      com.yahoo.ccdi.fetl.RcUtil.writeLong(this, dhrTag, indx++);
      com.yahoo.ccdi.fetl.RcUtil.writeLong(this, transformErrorTag, indx++);
      {
        com.yahoo.ccdi.fetl.RcUtil.writeInt(this, simpleFields.size(), indx++);
        System.out.println("### The number of elements in Map is " + simpleFields.size());
        java.util.Set<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_es1 = simpleFields.entrySet();
        for(java.util.Iterator<java.util.Map.Entry<String,org.apache.hadoop.record.Buffer>> _rio_midx1 = _rio_es1.iterator(); _rio_midx1.hasNext();) {
          java.util.Map.Entry<String,org.apache.hadoop.record.Buffer> _rio_me1 = _rio_midx1.next();
          String _rio_k1 = _rio_me1.getKey();
          org.apache.hadoop.record.Buffer _rio_v1 = _rio_me1.getValue();
          com.yahoo.ccdi.fetl.RcUtil.writeString(this, _rio_k1, indx++);
          com.yahoo.ccdi.fetl.RcUtil.writeBuffer(this, _rio_v1, indx++);
        }
      }
    } catch(java.io.IOException e) {
      e.printStackTrace();
    }
  }
  public void deserialize(org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable bra){
    try {
      int indx = 0;
      filterTag=com.yahoo.ccdi.fetl.RcUtil.readLong(bra, indx++);
      dhrTag=com.yahoo.ccdi.fetl.RcUtil.readLong(bra, indx++);
      transformErrorTag=com.yahoo.ccdi.fetl.RcUtil.readLong(bra, indx++);
      {
        int index = com.yahoo.ccdi.fetl.RcUtil.readInt(bra, indx++);
        System.out.println("The number of elements in Map is "+ index);
        simpleFields=new java.util.TreeMap<String,org.apache.hadoop.record.Buffer>();
        for ( int i = 0; i< index; i++) {
          String _rio_k1;
          _rio_k1=com.yahoo.ccdi.fetl.RcUtil.readString(bra, indx++);
          org.apache.hadoop.record.Buffer _rio_v1;
          _rio_v1=com.yahoo.ccdi.fetl.RcUtil.readBuffer(bra, indx++);
          simpleFields.put(_rio_k1,_rio_v1);
        }
      }
    } catch(java.io.IOException e) {
      e.printStackTrace();
    }
  }
}
